{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "authorship_tag": "ABX9TyNDEAVZtAjTOvJDqZwOPX2N"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Neural Causal Model\n",
        "\n",
        "As Robert Osazuwa Ness details in his book [causal ai](https://www.manning.com/books/causal-ai):\n",
        "\n",
        "> We regularly work with functions in causal modeling and inference, and sometimes it makes sense to approximate them, *so long as the approximations preserve the causal information we care about.*.. [i]n this section, we’ll do this mapping between a node and its parents with the variational autoencoder (VAE) framework. We’ll train two deep neural nets in the VAE, one of which *maps parent cause variables to a distribution of the outcome variable*, and another that *maps the outcome variable to a distribution of the cause variables.*\n",
        "\n",
        "We will use this framework to build a neural causal model of an image using the combined dataset of [MNIST](https://en.wikipedia.org/wiki/MNIST_database) and [TypefaceMNIST](https://paperswithcode.com/dataset/typography-mnist) in which a binary label represents whether the digit is handwritten(i.e., 1) or typed (i.e., 0)."
      ],
      "metadata": {
        "id": "JaTr-LfOZ3vR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jodaWWM1F39U"
      },
      "outputs": [],
      "source": [
        "!pip install \"pyro-ppl\"\n",
        "!pip install \"torch\"\n",
        "!pip install \"torchvision\"\n",
        "!pip install \"numpy==1.26.4\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Downloading and Preprocessing Data"
      ],
      "metadata": {
        "id": "ooFiAUnYfyBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "USE_CUDA = True\n",
        "DEVICE_TYPE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "metadata": {
        "id": "YmP6Xbubf4lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class to combine the datasets\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "class CombinedDataset(Dataset):\n",
        "  def __init__(self, csv_file):\n",
        "    self.dataset = pd.read_csv(csv_file)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    images = self.dataset.iloc[idx, 3:]\n",
        "    images = np.array(images, dtype=\"float32\")/255\n",
        "    images = images.reshape(28, 28)\n",
        "    transform = transforms.ToTensor()\n",
        "    images = transform(images)\n",
        "    digits = self.dataset.iloc[idx, 2]\n",
        "    digits = np.array([digits], dtype=\"int\")\n",
        "    is_handwritten = self.dataset.iloc[idx, 1]\n",
        "    is_handwritten = np.array([is_handwritten], dtype='float32')\n",
        "    return images, digits, is_handwritten\n"
      ],
      "metadata": {
        "id": "3DdrG6hsgXgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download, split, load data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "def setup_dataloaders(batch_size=64, use_cuda=USE_CUDA):\n",
        "  combined_dataset = CombinedDataset(\n",
        "      csv_file = \"https://raw.githubusercontent.com/altdeep/causalML/master/datasets/combined_mnist_tmnist_data.csv\"\n",
        "  )\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
        "  n = len(combined_dataset)\n",
        "  train_size = int(0.8 *n)\n",
        "  test_size = n-train_size\n",
        "  train_dataset, test_dataset = random_split(\n",
        "        combined_dataset,\n",
        "        [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "  train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs\n",
        "    )\n",
        "  test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs\n",
        "    )\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "n6ajIuOzoP9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating the Encoder and Decoder\n",
        "\n",
        "Similar to the traditional [Variational AutoEncoder (VAE)](https://en.wikipedia.org/wiki/Variational_autoencoder) used in machine learning the causal form is also composed of an encoder and decoder, but is used to model the *causal DAG of generating the image* (i.e.,the DGP that generates the MNST image). To that end we model the causal drivers of what determines the image we see. For this example, the main drivers of the MNST images are  1) the label/type of *digit* 2) the label of is it *handwritten* and 3) (and in my opinion the most important) the latent causes as represented by the variable Z. As Robert Osazuwa Ness details in his book [causal ai](https://www.manning.com/books/causal-ai):\n",
        "\n",
        ">  Z appears as a new parent in the causal DAG...we view *digit* and *is-handwritten* as causal drivers of what we see in the image. Yet there are other elements of the image (e.g., the stroke thickness of a handwritten character, or the font of a typed character) that are also causes of what we see in the image. *We’ll think of Z as a continuous latent stand-in for all of these other causes of the image that we are not explicitly modeling...it is important to remember that the representation we learn for Z is a stand-in for latent causes and is not the same as learning the actual latent causes.*\n",
        "\n",
        "Accordingly, the encoder's job is to encode the explicit variables of image, the label of *is-handwritten* and the *digit* label into a latent variable Z which when combined with the explicit variables and fed into the decoder generates the image we see.\n"
      ],
      "metadata": {
        "id": "L2shIVd5wIDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the Decoder\n",
        "from torch import nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28\n",
        "        digit_dim = 10\n",
        "        is_handwritten_dim = 1\n",
        "        self.softplus = nn.Softplus()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        encoding_dim = z_dim + digit_dim + is_handwritten_dim\n",
        "        self.fc1 = nn.Linear(encoding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, img_dim)\n",
        "\n",
        "    def forward(self, z, digit, is_handwritten):\n",
        "        input = torch.cat([z, digit, is_handwritten], dim=1)\n",
        "        hidden = self.softplus(self.fc1(input))\n",
        "        img_param = self.sigmoid(self.fc2(hidden))\n",
        "        return img_param\n",
        "\n",
        "# creating the Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28\n",
        "        digit_dim = 10\n",
        "        is_handwritten_dim = 1\n",
        "        self.softplus = nn.Softplus()\n",
        "        input_dim = img_dim + digit_dim + is_handwritten_dim\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
        "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
        "\n",
        "    def forward(self, img, digit, is_handwritten):\n",
        "        input = torch.cat([img, digit, is_handwritten], dim=1)\n",
        "        hidden = self.softplus(self.fc1(input))\n",
        "        z_loc = self.fc21(hidden)\n",
        "        z_scale = torch.exp(self.fc22(hidden))\n",
        "        return z_loc, z_scale"
      ],
      "metadata": {
        "id": "fGjW5o33wkIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Pyro Functions used to model the Causal Variational AutoEncoder(VAE)\n",
        "\n",
        "As detailed above the decoder represents modeling the causal [Markov kernel](https://math.stackexchange.com/questions/4594349/question-about-the-definition-of-markov-kernel); that is the conditional probability of $P(X | digit, is-handwritten, Z)$ where $X$ represents the image.  The latent variable Z, the label *digit* and the label *is-handwritten* are sampled from standard canonical distributions. In this case Z is sampled from a Normal distribution, the label *digit* is sampled from a Categorical distribution, and the label *is-handwritten* is sampled from a Bernoulli distribution.\n",
        "\n",
        "Additionally as the book details since we are modeling the joint probability distribution (i.e. $P(X, digit, is-handwritten, Z)$), the random variable X which represents the pixel values of the image needs to be accounted for too. Even though the pixel values are not technically binary outcomes, since they can take values from 0 to 255, we model them using the Bernoulli distribution due to ease of use. This helper method is named **model** in the VAE.\n",
        "\n",
        "To apply the model to the images in the training data the helper method **training_model** is used to condition the model on the data (i.e., the labels and images). Furthermore, we want to get good samples for the latent variable Z *(since it represents the image)* we implement a guide function that uses the encoder's output and canonical distributions to sample good values for Z. This helper method is named **training_guide** in the VAE.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cB7vLi1Z5uK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating VAE\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "\n",
        "dist.enable_validation(False)\n",
        "class VAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        z_dim=50,\n",
        "        hidden_dim=400,\n",
        "        use_cuda=USE_CUDA,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.setup_networks()\n",
        "\n",
        "    def setup_networks(self):\n",
        "        self.encoder = Encoder(self.z_dim, self.hidden_dim)\n",
        "        self.decoder = Decoder(self.z_dim, self.hidden_dim)\n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "    def model(self, data_size=1):\n",
        "      pyro.module(\"decoder\", self.decoder)\n",
        "      options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "      z_loc = torch.zeros(data_size, self.z_dim, **options)\n",
        "      z_scale = torch.ones(data_size, self.z_dim, **options)\n",
        "      z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "      p_digit = torch.ones(data_size, 10, **options)/10\n",
        "      digit = pyro.sample(\n",
        "          \"digit\",\n",
        "          dist.OneHotCategorical(p_digit)\n",
        "      )\n",
        "      p_is_handwritten = torch.ones(data_size, 1, **options)/2\n",
        "      is_handwritten = pyro.sample(\n",
        "          \"is_handwritten\",\n",
        "          dist.Bernoulli(p_is_handwritten).to_event(1)\n",
        "      )\n",
        "      img_param = self.decoder(z, digit, is_handwritten)\n",
        "      img = pyro.sample(\"img\", dist.Bernoulli(img_param).to_event(1))\n",
        "      return img, digit, is_handwritten\n",
        "\n",
        "    def training_model(self, img, digit, is_handwritten, batch_size):\n",
        "      conditioned_on_data = pyro.condition(\n",
        "          self.model,\n",
        "          data={\n",
        "            \"digit\": digit,\n",
        "            \"is_handwritten\": is_handwritten,\n",
        "            \"img\": img\n",
        "          }\n",
        "      )\n",
        "      with pyro.plate(\"data\", batch_size):\n",
        "          img, digit, is_handwritten = conditioned_on_data(batch_size)\n",
        "      return img, digit, is_handwritten\n",
        "\n",
        "    def training_guide(self, img, digit, is_handwritten, batch_size):\n",
        "      pyro.module(\"encoder\", self.encoder)\n",
        "      options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "      with pyro.plate(\"data\", batch_size):\n",
        "        z_loc, z_scale = self.encoder(img, digit, is_handwritten)\n",
        "        normal_dist = dist.Normal(z_loc, z_scale).to_event(1)\n",
        "        z = pyro.sample(\"Z\", normal_dist)"
      ],
      "metadata": {
        "id": "PKQ0hKMcNFX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training the Causal Variational Autoencoder(VAE)\n",
        "\n",
        "In addition to the traditional method of training VAEs i.e., minimizing the reconstruction error between the original and reconstructed/generated images, we also introduce a probabilistic approach to train the causal VAE. As Robert Osazuwa Ness details in his book [causal ai](https://www.manning.com/books/causal-ai):\n",
        "\n",
        "> In practice, solely minimizing reconstruction error leads to overfitting and other issues, so we’ll opt for a probabilistic approach: *given an image, we’ll use our guide function to sample a value of Z from $P(Z|image, is-handwritten, digit).$* [And] [t]hen we’ll plug that value into our model’s decoder....\n",
        "\n",
        "Unfortunately, we cannot directly sample from the conditional probability distribution of $P(Z|image, is-handwritten, digit)$; in other words it is intractable. The solution to this problem is to *approximate the conditional probability distribution by using tractable distributions;* this training procedure is known as [variational inference](https://ermongroup.github.io/cs228-notes/inference/variational/)\n",
        "\n",
        "We will use the previous guide function i.e., **training_guide** to approximate the conditional probability distribution of $P(Z|image, is-handwritten, digit)$ (which in the literature is represented by the letter Q/q). We denote the distribution that we are approximating as $Q(Z | X, is-handwritten, digit)$ which represents the [variational distribution](https://ai.stackexchange.com/questions/41834/what-exactly-is-meant-by-variational-distribution). Interestingly, *changing the weights of the encoder represents shifting the variational distribution.* As Robert Osazuwa Ness details in his book [causal ai](https://www.manning.com/books/causal-ai):\n",
        "\n",
        "> A change in the weights of the encoder represents a shifting of the variational distribution. Training will optimize the weights of the encoder such that the variational distribution shifts toward $P(Z|image, is-handwritten, digit).$\n",
        "\n",
        "Training through [variational inference](https://ermongroup.github.io/cs228-notes/inference/variational/) works by minimizing the [Kullback Leibler (KL) divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the two distributions and minimizing the negative [ELBO loss](https://en.wikipedia.org/wiki/Evidence_lower_bound). Doing so will indirectly minimize the KL divergence between $Q(Z | X, is-handwritten, digit)$ and $P(Z|image, is-handwritten, digit)$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wlw4euBo1Xo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# helper function plotting images\n",
        "def plot_image(img, title=None):     #1\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(img.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# helper function for reconstruct and viewing\n",
        "def reconstruct_img(vae, img, digit, is_hw, use_cuda=USE_CUDA):     #1\n",
        "    img = img.reshape(-1, 28 * 28)\n",
        "    digit = F.one_hot(torch.tensor(digit), 10)\n",
        "    is_hw = torch.tensor(is_hw).unsqueeze(0)\n",
        "    if use_cuda:\n",
        "        img = img.cuda()\n",
        "        digit = digit.cuda()\n",
        "        is_hw = is_hw.cuda()\n",
        "    z_loc, z_scale = vae.encoder(img, digit, is_hw)\n",
        "    z = dist.Normal(z_loc, z_scale).sample()\n",
        "    img_expectation = vae.decoder(z, digit, is_hw)\n",
        "    return img_expectation.squeeze().view(28, 28).detach()\n",
        "\n",
        "def compare_images(img1, img2):    #2\n",
        "    fig = plt.figure()\n",
        "    ax0 = fig.add_subplot(121)\n",
        "    plt.imshow(img1.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    plt.title('original')\n",
        "    ax1 = fig.add_subplot(122)\n",
        "    plt.imshow(img2.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    plt.title('reconstruction')\n",
        "    plt.show()\n",
        "\n",
        "# helper data processing functions for training\n",
        " def get_random_example(loader):    #1\n",
        "    random_idx = np.random.randint(0, len(loader.dataset))    #1\n",
        "    img, digit, is_handwritten = loader.dataset[random_idx]     #1\n",
        "    return img.squeeze(), digit, is_handwritten    #1\n",
        "\n",
        "def reshape_data(img, digit, is_handwritten):     #2\n",
        "    digit = F.one_hot(digit, 10).squeeze()     #2\n",
        "    img = img.reshape(-1, 28*28)     #2\n",
        "    return img, digit, is_handwritten     #2\n",
        "\n",
        "def generate_coded_data(vae, use_cuda=USE_CUDA):     #3\n",
        "    z_loc = torch.zeros(1, vae.z_dim)     #3\n",
        "    z_scale = torch.ones(1, vae.z_dim)     #3\n",
        "    z = dist.Normal(z_loc, z_scale).to_event(1).sample()     #3\n",
        "    p_digit = torch.ones(1, 10)/10     #3\n",
        "    digit = dist.OneHotCategorical(p_digit).sample()     #3\n",
        "    p_is_handwritten = torch.ones(1, 1)/2     #3\n",
        "    is_handwritten = dist.Bernoulli(p_is_handwritten).sample()    #3\n",
        "    if use_cuda:     #3\n",
        "        z = z.cuda() #3\n",
        "        digit = digit.cuda() #3\n",
        "        is_handwritten = is_handwritten.cuda()     #3\n",
        "    img = vae.decoder(z, digit, is_handwritten)     #3\n",
        "    return img, digit, is_handwritten    #3\n",
        "\n",
        "def generate_data(vae, use_cuda=USE_CUDA):     #4\n",
        "    img, digit, is_handwritten = generate_coded_data(vae, use_cuda)    #4\n",
        "    img = img.squeeze().view(28, 28).detach()    #4\n",
        "    digit = torch.argmax(digit, 1)   #4\n",
        "    is_handwritten = torch.argmax(is_handwritten, 1)     #4\n",
        "    return img, digit, is_handwritten     #4"
      ],
      "metadata": {
        "id": "UXX39FUl13uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up training\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "pyro.clear_param_store()     #1\n",
        "vae = VAE()    #2\n",
        "train_loader, test_loader = setup_dataloaders(batch_size=256)    #3\n",
        "svi_adam = Adam({\"lr\": 1.0e-3})     #4\n",
        "model = vae.training_model    #5\n",
        "guide = vae.training_guide    #5\n",
        "svi = SVI(model, guide, svi_adam, loss=Trace_ELBO())     #5\n",
        "\n",
        "# setting up evaluation tester\n",
        "def test_epoch(vae, test_loader):\n",
        "    epoch_loss_test = 0     #1\n",
        "    for img, digit, is_hw in test_loader:    #1\n",
        "        batch_size = img.shape[0]    #1\n",
        "        if USE_CUDA:    #1\n",
        "            img = img.cuda()    #1\n",
        "            digit = digit.cuda()     #1\n",
        "            is_hw = is_hw.cuda()  #1\n",
        "        img, digit, is_hw = reshape_data(     #1\n",
        "            img, digit, is_hw     #1\n",
        "        )   #1\n",
        "        epoch_loss_test += svi.evaluate_loss(     #1\n",
        "            img, digit, is_hw, batch_size    #1\n",
        "        )    #1\n",
        "    test_size = len(test_loader.dataset)    #1\n",
        "    avg_loss = epoch_loss_test/test_size     #1\n",
        "    print(\"Epoch: {} avg. test loss: {}\".format(epoch, avg_loss))     #1\n",
        "    print(\"Comparing a random test image to its reconstruction:\")    #2\n",
        "    random_example = get_random_example(test_loader)     #2\n",
        "    img_r, digit_r, is_hw_r = random_example    #2\n",
        "    img_recon = reconstruct_img(vae, img_r, digit_r, is_hw_r)     #2\n",
        "    compare_images(img_r, img_recon)     #2\n",
        "    print(\"Generate a random image from the model:\")    #3\n",
        "    img_gen, digit_gen, is_hw_gen = generate_data(vae)    #3\n",
        "    plot_image(img_gen, \"Generated Image\")     #3\n",
        "    print(\"Intended digit: \", int(digit_gen))    #3\n",
        "    print(\"Intended as handwritten: \", bool(is_hw_gen == 1))     #3"
      ],
      "metadata": {
        "id": "G2dJS7ZTLM3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the training and evaluation loop\n",
        "\n",
        "NUM_EPOCHS = 2500\n",
        "TEST_FREQUENCY = 10\n",
        "\n",
        "train_loss = []\n",
        "train_size = len(train_loader.dataset)\n",
        "\n",
        "for epoch in range(0, NUM_EPOCHS+1):   #1\n",
        "    loss = 0\n",
        "    for img, digit, is_handwritten in train_loader:\n",
        "        batch_size = img.shape[0]\n",
        "        if USE_CUDA:\n",
        "            img = img.cuda()\n",
        "            digit = digit.cuda()\n",
        "            is_handwritten = is_handwritten.cuda()\n",
        "        img, digit, is_handwritten = reshape_data(\n",
        "            img, digit, is_handwritten\n",
        "        )\n",
        "        loss += svi.step(    #2\n",
        "            img, digit, is_handwritten, batch_size     #2\n",
        "        )     #2\n",
        "    avg_loss = loss / train_size\n",
        "    print(\"Epoch: {} avgs training loss: {}\".format(epoch, loss))\n",
        "    train_loss.append(avg_loss)\n",
        "    if epoch % TEST_FREQUENCY == 0:    #3\n",
        "        test_epoch(vae, test_loader)    #3\n"
      ],
      "metadata": {
        "id": "ojB5axgOLvj8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "authorship_tag": "ABX9TyPQESlVpBJ6YWsf4hXR7v/C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Neural Causal Model\n",
        "\n",
        "As Robert Osazuwa Ness details in his book [causal ai](https://www.manning.com/books/causal-ai):\n",
        "\n",
        "> We regularly work with functions in causal modeling and inference, and sometimes it makes sense to approximate them, *so long as the approximations preserve the causal information we care about.*.. [i]n this section, we’ll do this mapping between a node and its parents with the variational autoencoder (VAE) framework. We’ll train two deep neural nets in the VAE, one of which *maps parent cause variables to a distribution of the outcome variable*, and another that *maps the outcome variable to a distribution of the cause variables.*\n",
        "\n",
        "We will use this framework to build a neural causal model of an image using the combined dataset of [MNIST](https://en.wikipedia.org/wiki/MNIST_database) and [TypefaceMNIST](https://paperswithcode.com/dataset/typography-mnist) in which a binary label represents whether the digit is handwritten(i.e., 1) or typed (i.e., 0)."
      ],
      "metadata": {
        "id": "JaTr-LfOZ3vR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jodaWWM1F39U"
      },
      "outputs": [],
      "source": [
        "!pip install \"pyro-ppl\"\n",
        "!pip install \"torch\"\n",
        "!pip install \"torchvision\"\n",
        "!pip install \"numpy==1.26.4\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Downloading and Preprocessing Data"
      ],
      "metadata": {
        "id": "ooFiAUnYfyBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "USE_CUDA = True\n",
        "DEVICE_TYPE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "metadata": {
        "id": "YmP6Xbubf4lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class to combine the datasets\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "class CombinedDataset(Dataset):\n",
        "  def __init__(self, csv_file):\n",
        "    self.dataset = pd.read_csv(csv_file)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    images = self.dataset.iloc[idx, 3:]\n",
        "    images = np.array(images, dtype=\"float32\")/255\n",
        "    images = images.reshape(28, 28)\n",
        "    transform = transforms.ToTensor()\n",
        "    images = transform(images)\n",
        "    digits = self.dataset.iloc[idx, 2]\n",
        "    digits = np.array([digits], dtype=\"int\")\n",
        "    is_handwritten = self.dataset.iloc[idx, 1]\n",
        "    is_handwritten = np.array([is_handwritten], dtype='float32')\n",
        "    return images, digits, is_handwritten\n"
      ],
      "metadata": {
        "id": "3DdrG6hsgXgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download, split, load data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "def setup_dataloaders(batch_size=64, use_cuda=USE_CUDA):\n",
        "  combined_dataset = CombinedDataset(\n",
        "      csv_file = \"https://raw.githubusercontent.com/altdeep/causalML/master/datasets/combined_mnist_tmnist_data.csv\"\n",
        "  )\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
        "  n = len(combined_dataset)\n",
        "  train_size = int(0.8 *n)\n",
        "  test_size = n-train_size\n",
        "  train_dataset, test_dataset = random_split(\n",
        "        combined_dataset,\n",
        "        [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "  train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs\n",
        "    )\n",
        "  test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs\n",
        "    )\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "n6ajIuOzoP9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating the Encoder and Decoder\n",
        "\n",
        "Similar to the traditional [Variational AutoEncoder (VAE)](https://en.wikipedia.org/wiki/Variational_autoencoder) used in machine learning the causal form is also composed of an encoder and decoder, but is used to model the *causal DAG of generating the image* (i.e.,the DGP that generates the MNST image). To that end we model the causal drivers of what determines the image we see. For this example, the main drivers of the MNST images are  1) the label/type of *digit* 2) the label of is it *handwritten* and 3) (and in my opinion the most important) the latent causes as represented by the variable Z. As Robert Osazuwa Ness details in his book [causal ai](https://www.manning.com/books/causal-ai):\n",
        "\n",
        ">  Z appears as a new parent in the causal DAG...we view *digit* and *is-handwritten* as causal drivers of what we see in the image. Yet there are other elements of the image (e.g., the stroke thickness of a handwritten character, or the font of a typed character) that are also causes of what we see in the image. *We’ll think of Z as a continuous latent stand-in for all of these other causes of the image that we are not explicitly modeling...it is important to remember that the representation we learn for Z is a stand-in for latent causes and is not the same as learning the actual latent causes.*\n",
        "\n",
        "Accordingly, the encoder's job is to encode the explicit variables of image, the label of *is-handwritten* and the *digit* label into a latent variable Z which when combined with the explicit variables and fed into the decoder generates the image we see.\n"
      ],
      "metadata": {
        "id": "L2shIVd5wIDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the Decoder\n",
        "from torch import nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28\n",
        "        digit_dim = 10\n",
        "        is_handwritten_dim = 1\n",
        "        self.softplus = nn.Softplus()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        encoding_dim = z_dim + digit_dim + is_handwritten_dim\n",
        "        self.fc1 = nn.Linear(encoding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, img_dim)\n",
        "\n",
        "    def forward(self, z, digit, is_handwritten):\n",
        "        input = torch.cat([z, digit, is_handwritten], dim=1)\n",
        "        hidden = self.softplus(self.fc1(input))\n",
        "        img_param = self.sigmoid(self.fc2(hidden))\n",
        "        return img_param\n",
        "\n",
        "# creating the Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28\n",
        "        digit_dim = 10\n",
        "        is_handwritten_dim = 1\n",
        "        self.softplus = nn.Softplus()\n",
        "        input_dim = img_dim + digit_dim + is_handwritten_dim\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
        "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
        "\n",
        "    def forward(self, img, digit, is_handwritten):\n",
        "        input = torch.cat([img, digit, is_handwritten], dim=1)\n",
        "        hidden = self.softplus(self.fc1(input))\n",
        "        z_loc = self.fc21(hidden)\n",
        "        z_scale = torch.exp(self.fc22(hidden))\n",
        "        return z_loc, z_scale"
      ],
      "metadata": {
        "id": "fGjW5o33wkIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Pyro Functions used to model the Causal Variational AutoEncoder(VAE)\n",
        "\n",
        "As detailed above the decoder represents modeling the causal [Markov kernel](https://math.stackexchange.com/questions/4594349/question-about-the-definition-of-markov-kernel); that is the conditional probability of $P(X | digit, is-handwritten, Z)$ where $X$ represents the image.  The latent variable Z, the label *digit* and the label *is-handwritten* are sampled from standard canonical distributions. In this case Z is sampled from a Normal distribution, the label *digit* is sampled from a Categorical distribution, and the label *is-handwritten* is sampled from a Bernoulli distribution.\n",
        "\n",
        "Additionally as the book details since we are modeling the joint probability distribution (i.e. $P(X, digit, is-handwritten, Z)$), the random variable X which represents the pixel values of the image needs to be accounted for too. Even though the pixel values are not technically binary outcomes, since they can take values from 0 to 255, we model them using the Bernoulli distribution due to ease of use. This helper method is named **model** in the VAE.\n",
        "\n",
        "To apply the model to the images in the training data the helper method **training_model** is used to condition the model on the data (i.e., the labels and images). Furthermore, we want to get good samples for the latent variable Z *(since it represents the image)* we implement a guide function that uses the encoder's output and canonical distributions to sample good values for Z. This helper method is named **training_guide** in the VAE.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cB7vLi1Z5uK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating VAE\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "\n",
        "dist.enable_validation(False)\n",
        "class VAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        z_dim=50,\n",
        "        hidden_dim=400,\n",
        "        use_cuda=USE_CUDA,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.setup_networks()\n",
        "\n",
        "    def setup_networks(self):\n",
        "        self.encoder = Encoder(self.z_dim, self.hidden_dim)\n",
        "        self.decoder = Decoder(self.z_dim, self.hidden_dim)\n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "    def model(self, data_size=1):\n",
        "      pyro.module(\"decoder\", self.decoder)\n",
        "      options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "      z_loc = torch.zeros(data_size, self.z_dim, **options)\n",
        "      z_scale = torch.ones(data_size, self.z_dim, **options)\n",
        "      z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "      p_digit = torch.ones(data_size, 10, **options)/10\n",
        "      digit = pyro.sample(\n",
        "          \"digit\",\n",
        "          dist.OneHotCategorical(p_digit)\n",
        "      )\n",
        "      p_is_handwritten = torch.ones(data_size, 1, **options)/2\n",
        "      is_handwritten = pyro.sample(\n",
        "          \"is_handwritten\",\n",
        "          dist.Bernoulli(p_is_handwritten).to_event(1)\n",
        "      )\n",
        "      img_param = self.decoder(z, digit, is_handwritten)\n",
        "      img = pyro.sample(\"img\", dist.Bernoulli(img_param).to_event(1))\n",
        "      return img, digit, is_handwritten\n",
        "\n",
        "    def training_model(self, img, digit, is_handwritten, batch_size):\n",
        "      conditioned_on_data = pyro.condition(\n",
        "          self.model,\n",
        "          data={\n",
        "            \"digit\": digit,\n",
        "            \"is_handwritten\": is_handwritten,\n",
        "            \"img\": img\n",
        "          }\n",
        "      )\n",
        "      with pyro.plate(\"data\", batch_size):\n",
        "          img, digit, is_handwritten = conditioned_on_data(batch_size)\n",
        "      return img, digit, is_handwritten\n",
        "\n",
        "    def training_guide(self, img, digit, is_handwritten, batch_size):\n",
        "      pyro.module(\"encoder\", self.encoder)\n",
        "      options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "      with pyro.plate(\"data\", batch_size):\n",
        "        z_loc, z_scale = self.encoder(img, digit, is_handwritten)\n",
        "        normal_dist = dist.Normal(z_loc, z_scale).to_event(1)\n",
        "        z = pyro.sample(\"Z\", normal_dist)"
      ],
      "metadata": {
        "id": "PKQ0hKMcNFX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training the Causal Variational Autoencoder(VAE)"
      ],
      "metadata": {
        "id": "Wlw4euBo1Xo5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXX39FUl13uI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
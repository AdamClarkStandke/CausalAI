{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oePJH_2uTvvZ",
        "f3JmWeEbfoo0"
      ],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPrJBVHqiSgawtxCfVMYi6F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w00yrpPZ0Bi"
      },
      "outputs": [],
      "source": [
        "!pip install pgmpy\n",
        "!pip install pyro-ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding random processes\n",
        "Want to model the following joint probability distribution $P(x, y, z)$ assuming Y is conditionally independent of Z given X:\n",
        "1.   Apply Chain Rule: $P(x, y, z)=P(z)P(x|z)P(y|x, z)$\n",
        "2.   Apply Conditional Independence: $P(x, y, z)=P(z)P(x|z)P(y|x)$\n",
        "3.   Sample from probability distributions:\n",
        "      1.   $z\\sim P(Z)$\n",
        "      2.   $x\\sim P(X|Z=z)$\n",
        "      3.   $y\\sim P(Y|X=x)$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oePJH_2uTvvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmpy.factors.discrete.CPD import TabularCPD\n",
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "from pgmpy.sampling import BayesianModelSampling"
      ],
      "metadata": {
        "id": "Or7GEsYET1kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random process using pgmpy\n",
        "# creating probability dirstributions\n",
        "PZ = TabularCPD(     #1\n",
        "    variable='Z',     #1\n",
        "    variable_card=2,     #1\n",
        "    values=[[.65], [.35]],     #1\n",
        "    state_names = {     #1\n",
        "        'Z': ['0', '1']     #1\n",
        "    })     #1\n",
        "\n",
        "PXgivenZ = TabularCPD(     #2\n",
        "    variable='X',    #2\n",
        "    variable_card=2,   #2\n",
        "    values=[   #2\n",
        "        [.8, .6],   #2\n",
        "        [.2, .4],   #2\n",
        "    ],    #2\n",
        "    evidence=['Z'],    #2\n",
        "    evidence_card=[2],  #2\n",
        "    state_names = {    #2\n",
        "        'X': ['0', '1'],    #2\n",
        "        'Z': ['0', '1'] #2\n",
        "    })   #2\n",
        "\n",
        "PYgivenX = TabularCPD(     #3\n",
        "    variable='Y',     #3\n",
        "    variable_card=3,     #3\n",
        "    values=[    #3\n",
        "        [.1, .8],     #3\n",
        "        [.2, .1],     #3\n",
        "        [.7, .1],    #3\n",
        "    ],    #3\n",
        "    evidence=['X'],     #3\n",
        "    evidence_card=[2],    #3\n",
        "    state_names = {    #3\n",
        "        'Y': ['1', '2', '3'],    #3\n",
        "        'X': ['0', '1']    #3\n",
        "    })    #3\n",
        "# generating samples for x, y z\n",
        "model = DiscreteBayesianNetwork([('Z', 'X'), ('X', 'Y')])\n",
        "model.add_cpds(PZ, PXgivenZ, PYgivenX)\n",
        "generator = BayesianModelSampling(model)\n",
        "generator.forward_sample(size=1)"
      ],
      "metadata": {
        "id": "3jmtoSd8UhfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random process using pyro\n",
        "import torch\n",
        "from pyro.distributions import Bernoulli, Poisson, Gamma\n",
        "\n",
        "z = Gamma(7.5, 1.0).sample()    #1\n",
        "x = Poisson(z).sample()    #2\n",
        "y = Bernoulli(x / (5+x)).sample()   #3\n",
        "print(z, x, y)"
      ],
      "metadata": {
        "id": "drkkRqBklgtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyro\n",
        "def random_process():\n",
        "    z = pyro.sample(\"z\", Gamma(7.5, 1.0))\n",
        "    x = pyro.sample(\"x\", Poisson(z))\n",
        "    y = torch.tensor(0.0)\n",
        "    for i in range(int(x)):\n",
        "        y += pyro.sample(f\"y{i}\", Bernoulli(.5))    #1\n",
        "    return y\n",
        "#print(random_process())\n",
        "generated_samples = torch.stack([random_process() for _ in range(100)])\n",
        "generated_samples.mean()"
      ],
      "metadata": {
        "id": "GX3Ctt8QtJRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buiding a Causal DAG\n",
        "\n",
        "Will be building a causal model using a causal DAG of people's choice of transportation on their daily commute with the following random variables and causal relationships/assumptions:\n",
        "\n",
        "## Random Variables:\n",
        "1.   *A* = The age of an individual\n",
        "2.   *S* = An individuals reported gender\n",
        "3.   *E* = An individual's level of education or training\n",
        "4.   *O* = An individuals occupation\n",
        "5.   *R* = The size of the city the individual resides in\n",
        "6.   *T* = The means of transport favored by the individual\n",
        "\n",
        "## Causal Relationships/Assumptions:\n",
        "*   age is a cause of education (A→E)\n",
        "*   gender is a cause of education (S→E)\n",
        "*   education is a cause of occupation (E→O)\n",
        "*   education is a cause of where people reside (E→R)\n",
        "*   occupation is a cause of transportation (O→T)\n",
        "*   residence is a cause of transportation (R→T)"
      ],
      "metadata": {
        "id": "f3JmWeEbfoo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "import networkx as nx\n",
        "# Building list of causal relationships\n",
        "caus_real=[('A', 'E'),('S', 'E'),('E', 'O'),('E', 'R'),('O', 'T'),('R', 'T')]\n",
        "# Building causal DAG from causal relationships\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from(caus_real)\n",
        "model = DiscreteBayesianNetwork(ebunch=G)"
      ],
      "metadata": {
        "id": "jOJ2x4Ecmfkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Bayesian Network based on a Causal DAG\n",
        "\n",
        "Will be building a probabilistic machine learning model (which in this case, is a Bayesian Network) for modelling people's choices for transportation on their daily commutes. We will use the previous random variables and causal relationships/assumptions as detailed in the previous cells with additional information added to the modeling process; namely, DATA! As detailed by [Robert Osazuwa Ness](https://www.amazon.com/Causal-AI-Robert-Ness/dp/1633439917), the data comes from a survey covering 500 individuals and is encoded in categorical form as follows:\n",
        "\n",
        "## Recorded Survey Data:\n",
        "1.   *A* = Recorded as **young** (“young”) for individuals up to and including 29 years, **adult** (“adult”) for individuals between 30 and 60 years old (inclusive), and **old **(“old”) for people 61 and over\n",
        "2.   *S* = The self-reported gender of an individual, recorded as **male** (“M”), **female** (“F”), or **other** (“O”)\n",
        "3.   *E* = The highest level of education or training completed by the individual, recorded either **high school** (“high”) or **university degree** (“uni”)\n",
        "4.   *O* = **Employee** (“emp”) or a **self-employed** worker (“self”)\n",
        "5.   *R* = The population size of the city the individual lives in, recorded as **small** (“small”) or **big** (“big”)\n",
        "6.   *T* = The means of transport favored by the individual, recorded as **car** (“car”), **train** (“train”), or **other** (“other”)\n",
        "\n",
        "## Parameter/Kernel Estimation:\n",
        "Given the data, we want to model the following joint probability distribution: $P(A, S, E, O, R, T)$ which through factorization(i.e. chain rule, conditional independence) is equivalent to the following: $P(A)P(S)P(E|S, A)P(O|E)P(R|E)P(T|O, R)$. These factors are called *causal Markov Kernels* and we want to estimate these kernels using the data. This is done through traditional parameter estimation techniques such as [Maximum Likelihood Estimation(MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation), [Bayesian Estimation](https://en.wikipedia.org/wiki/Bayes_estimator#:~:text=In%20estimation%20theory%20and%20decision,expectation%20of%20a%20utility%20function.), and [expectation maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) using our causal DAG encoded as a [Bayesian Network](https://en.wikipedia.org/wiki/Bayesian_network)."
      ],
      "metadata": {
        "id": "c7vjrP5si2O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the collected transportation data\n",
        "import pandas as pd\n",
        "url='https://raw.githubusercontent.com/altdeep/causalML/master/datasets/transportation_survey.csv'\n",
        "data = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "8wTRYZ9995rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Creating simple bar plot of certain variables\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "fig, axs = plt.subplots(3, 1)\n",
        "data.groupby('A').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'), ylabel=\"Age\", ax=axs[0])\n",
        "data.groupby('O').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'), ylabel=\"Job\", ax=axs[1])\n",
        "data['T'].value_counts().reindex(['train','car', 'other'], fill_value=0).plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'),ylabel=\"Transportation\", ax=axs[2])"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "9JTKLvkb_Mlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "As detailed by [Robert Osazuwa Ness](https://www.amazon.com/Causal-AI-Robert-Ness/dp/1633439917): *MLE seeks the parameters/kernels that maximizes the likelihood of seeing the data we are using to train the model*. Which in our case is a Bayesian Network. Because the data we are using consists of categorical data, MLE calculates the probability distributions/Markov kernels by the proportion of counts in the data that staisfy a given critera. So for the CPD/kernel of $P(O=emp | E=high)$ it is calcualted as follows: $ \\frac{\\sum_{i}^{n}(O=emp\\; and \\;  E=high)}{\\sum_{i}^{n}E=high}$"
      ],
      "metadata": {
        "id": "j-eSB7KeW1BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using mle to get Markov kernels\n",
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "import networkx as nx\n",
        "# Building list of causal relationships\n",
        "caus_real=[('A', 'E'),('S', 'E'),('E', 'O'),('E', 'R'),('O', 'T'),('R', 'T')]\n",
        "# Building causal DAG from causal relationships\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from(caus_real)\n",
        "model_mle = DiscreteBayesianNetwork(ebunch=G)\n",
        "model_mle.fit(data=data, estimator=MaximumLikelihoodEstimator)\n",
        "causal_markov_kernels = model_mle.get_cpds()\n",
        "# displaying the CPD/Markov kernel for P(T| O, R)\n",
        "cmk_T = causal_markov_kernels[1]\n",
        "df = cmk_T.to_dataframe()\n",
        "df"
      ],
      "metadata": {
        "id": "1kXKeWVX-GTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Estimation\n",
        "\n",
        "The Bayesian Estimation that is used by the [pgmpy](https://pgmpy.org/index.html) library relies upon using conjugate priors to calculate the posterior distribution.Pgmpy implements a [Dirichlet conjugate prior](https://en.wikipedia.org/wiki/Dirichlet_distribution) for categorical outcomes which makes sense since the Dirichlet distribution is the conjugate prior distribution of the categorical distribution which means that if a data point has a categorical distribution, and the prior distribution is distributed as a Dirichlet, then the posterior distribution of the parameters/kernels are also Dirichlet. This form of estimation will act like a smoothing mechanism to help alleviate biased/incomplete data in which extreme probability outcomes may occur (e.g., under MLE the probablity of taking a car in your daily commute when self-employed and living in a small region is 100% which is probably not true)"
      ],
      "metadata": {
        "id": "9fZcwOQqW_LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using Bayes Estimation to get Markov kernels\n",
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "from pgmpy.estimators import BayesianEstimator\n",
        "import networkx as nx\n",
        "# Building list of causal relationships\n",
        "caus_real=[('A', 'E'),('S', 'E'),('E', 'O'),('E', 'R'),('O', 'T'),('R', 'T')]\n",
        "# Building causal DAG from causal relationships\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from(caus_real)\n",
        "model = DiscreteBayesianNetwork(ebunch=G)\n",
        "model.fit(data=data, estimator=BayesianEstimator,\n",
        "          prior_type=\"dirichlet\",\n",
        "          pseudo_counts=1)\n",
        "causal_markov_kernels = model.get_cpds()\n",
        "# displaying the CPD/Markov kernel for P(T| O, R)\n",
        "cmk_T = causal_markov_kernels[1]\n",
        "df = cmk_T.to_dataframe()\n",
        "df"
      ],
      "metadata": {
        "id": "SzvMFdZZLsWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expectation Maximization (EM)\n",
        "\n",
        "The Expectation Maximization (EM) algorithm is an iterative method to find the  maximum likelihood estimates (MLE) of parameters/kernels, *where the model depends on unobserved latent variables*. The EM algorithm alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters/kernels, and a maximization (M) step, which computes parameters/kernels maximizing the expected log-likelihood found in the E step. These parameter/kernel estimates are then used to determine the distribution of the latent variables in the next E step."
      ],
      "metadata": {
        "id": "yxZcM8LSXDwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmpy.estimators import ExpectationMaximization as EM\n",
        "# removing the random variable E in the data\n",
        "data_sans_E = data[['A', 'S', 'O', 'R', 'T']]\n",
        "model_with_latent = DiscreteBayesianNetwork(\n",
        "       [\n",
        "        ('A', 'E'),\n",
        "        ('S', 'E'),\n",
        "        ('E', 'O'),\n",
        "        ('E', 'R'),\n",
        "        ('O', 'T'),\n",
        "        ('R', 'T')\n",
        "     ],\n",
        "     latents={\"E\"}    #3 variable that is latent\n",
        ")\n",
        "estimator = EM(model_with_latent, data_sans_E)\n",
        "cmks_with_latent = estimator.get_parameters(latent_card={'E': 2})\n",
        "cmks_with_latent[3].to_dataframe()"
      ],
      "metadata": {
        "id": "Jxg7UtpqJfsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Inferences using trained Bayesian Network\n",
        "\n",
        "After the probabilistic machine learning model has been trained, it can be used for making inferences using inference algorithms to infer the CPD/Markov kernels of an outcome for any subset of the variables, given outcomes for the other variables. The inference algorithm used in this example for a Bayesian Network is [variable elimination](https://en.wikipedia.org/wiki/Variable_elimination). In this example we will use the variable elimination algorithm to infer the following CPD/Markov Kernel: $P(E|T)\\;when\\;T=train\\; and\\;T=car$ using the mle and Bayesian estimation methods."
      ],
      "metadata": {
        "id": "I-wJwGW6U-8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmpy.inference import VariableElimination\n",
        "#mle\n",
        "inference = VariableElimination(model_mle)\n",
        "query1 = inference.query(['E'], evidence={\"T\": \"train\"})\n",
        "query2 = inference.query(['E'], evidence={\"T\": \"car\"})\n",
        "print(\"train_mle\")\n",
        "print(query1)\n",
        "print(\"car_mle\")\n",
        "print(query2)\n",
        "print(\"----------------------------\")\n",
        "#bayesian\n",
        "inference = VariableElimination(model)\n",
        "query1 = inference.query(['E'], evidence={\"T\": \"train\"})\n",
        "query2 = inference.query(['E'], evidence={\"T\": \"car\"})\n",
        "print(\"train_bayes\")\n",
        "print(query1)\n",
        "print(\"car_bayes\")\n",
        "print(query2)"
      ],
      "metadata": {
        "id": "r_M74jy0feRA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}